The proposed system is a machine learning based system, which takes in a fundus photograph and predicts an output as would a real opthamologist. The system preprocesses the image to remove irregularities and produce consistent images. This is required as machine learning algorithms expect uniform input. The images are also reduced in size and channels, which reduces the memory footprint and increases training speed and number of input parameters. Feature extraction was used to extract important features from the image. These features are descriptive of the images and are better than using pixel values as features. They reduce the size of the input vector by as huge amount. Another method to reduce the input space, is by using dimensionality reduction techniques. These techniques extract only the most significant features from the input subspace. These extracted features are "almost" as significant as the whole input space. Training on these extracted features is very fast and as we have seen, improves accuracy and model performance. We used Linear Discriminant Analysis to perform dimensionality reduction. We used a number of machine learning and deep learning algorithms for the actual training. Machine Learning algorithms include Naive Bayes Classifier, Random Forest Classifier and Support Vector Machines and deep learning algorithms include ConvNets or Convolutional Neural Networks. These algorithms are considered "state-of-the-art" in their respective domains. Unlike Naive Bayes, Random Forests and SVMs, ConvNets extract, or learn to extract their own features and hence are considered "the best" for image classification problems. We managed to get good results using the three machine learning algorithms. However due to less hardware resources i.e. GPU we could not train ConvNets as much as we wanted and hence could only get sub-par results usiing ConvNets. We applied cross-validation techniques to the machine learning algorithms. For Naive Bayes Classifier, we used k-fold Cross Validation. This technique partitions the training data into k partitions and then trains on (k-1) partitions and tests with 1 partition. This process is repeated k times and the variance of the model is tested. Random Forest Classifier and SVMs have various hyper-parameters. We can manipulate the performance of the model by varying these hyper-parameters. So, in order to search for the best parameters for the system, we used a technique known as Grid Search Cross Validation. This technique creates a hyper-cube of each hyper-parameter as a dimension and then tests each and every possible combination of values and see which one works best. This is an exhaustive search method. We found the best parameters which work for RF and SVM with and without LDA. Hence the observation that RF and SVM perform well with and without LDA, unlike Naive Bayes, which received a major boost in accuracy because of dimensionality reduction. Most of the models benefit from more data, but beyond the point of approximately 5000-10000 images, there are no dramatic changes and hence we tested the model till 10000 images. ConvNets of the other hand were trained on the whole dataset as they need a large amount of data to generalize well. They also benefit from deeper nets i.e. more layers and people have tested various architectures and found success with it. However, due to limitations in the hardware we own, we could not try many different variations of ConvNets and hyper-parameters and hence had to abandon ConvNets after a single attempt. We also created a GUI application to apply these models for the actual task of prediction of images. The user is able to use a simple interface to load and rate images.

Some of the general observations were:

1. Histogram of Oriented Gradients provides good features for the task of classification

2. We can improve the performance of the models by applying dimensionality reduction techniques to the input features.

3. However only models without hyper-parameters benefit significantly from it.

4. Algorithms which have hyper-parameters can benefit by searching for the best set of hyper-parameters for the particular problem. This leads to a very significant boost in performance.

5. Random Forest and Support Vector Machines work best for the task, followed by Naive Bayes Classifier.

6. ConvNets should ideally work better for the task of image classification. 
